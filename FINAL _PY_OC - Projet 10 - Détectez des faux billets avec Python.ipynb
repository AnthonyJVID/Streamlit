# -*- coding: utf-8 -*-
"""
Created on Fri Sep 15 17:49:09 2023

@author: Anthony
"""

# Projet 10 - Détectez des faux billets avec R ou Python

#Importation des librairies

# Base
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns


# Sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error
from sklearn.impute import KNNImputer
from sklearn.metrics import silhouette_score
from sklearn.metrics import silhouette_samples

# Scipy
from scipy.stats import t, shapiro

# Statsmodels
import statsmodels
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.formula.api import ols

# Interface utilisateur
import streamlit as st

df = pd.read_csv("billets.csv", sep = ";", decimal = ".")

df.head()

df.tail()

df.sample(20)

df.info()

df.dtypes.value_counts()

df.nunique()

df.duplicated().sum()

df.isnull().sum()

df.describe(include = "all")

df.duplicated().sum()

## Gestion des valeurs manquante

##### 37 valeurs manquante dans la colonne "margin_low"

#### Utilisation de la Régression Linéaire pour estimer les valeurs manquantes

##### Dans le contexte de recherche de valeurs manquantes, la régression linéaire peut être utilisée pour estimer les valeurs manquantes en se basant sur la relation linéaire entre les variables connues.

##### En utilisant les valeurs disponibles, elle établit une équation de la droite de régression et prédit les valeurs manquantes en fonction de cette relation. Cela permet de compléter les données manquantes de manière cohérente en se basant sur les informations existantes.

## Regression Linéaire

reg_multi = smf.ols('margin_low ~ is_genuine + diagonal + height_left + height_right + margin_up + length', data = df).fit()
print (reg_multi.summary())
sm.stats.anova_lm(reg_multi, typ=3)

##### Certains paramètres ne sont pas significativement différents de 0, car leur p-valeur n'est pas inférieure à 5 %

##### Je retire les variables non significatives. 

##### Je commence par la colonne la moins significative "diagonal" car sa p-value = 0.71

reg_multi = smf.ols('margin_low ~ is_genuine + height_left + height_right + margin_up + length', data = df).fit()
print (reg_multi.summary())
sm.stats.anova_lm(reg_multi, typ=3)

##### C'est maintenant "length", avec une p-valeur de 0.88, qui est la moins significative.

reg_multi = smf.ols('margin_low ~ is_genuine + height_left + height_right + margin_up', data = df).fit()
print (reg_multi.summary())
sm.stats.anova_lm(reg_multi, typ=3)

##### Je retire "height_right" qui a une p-valeur de 0.5.

reg_multi = smf.ols('margin_low ~ is_genuine + height_left + margin_up', data = df).fit()
print (reg_multi.summary())
sm.stats.anova_lm(reg_multi, typ=3)

##### Je retire "height_left" qui a une p-valeur de 0.45.

reg_multi = smf.ols('margin_low ~ is_genuine + margin_up', data = df).fit()
print (reg_multi.summary())
sm.stats.anova_lm(reg_multi, typ=2)

##### Tous les paramètres sont significatifs (p-values proches de 0).

##### Le R-squared (coefficient de détermination) vaut environ 0.62, tout comme le Adj. R-squared (coefficient de détermination ajusté).

# Création d'un nouveau df "reg_lin"
reg_lin = df[["is_genuine", "margin_up"]].copy()
reg_lin.head()

# Création de la variable pour la méthode de prédiction "predict"
var_margin_low = reg_multi.predict(reg_lin)

# Utilisation de la variable "var_margin_low" pour remplacer les valeurs manquantes
df["margin_low"].fillna(var_margin_low, inplace = True)
df.isnull().sum()

# Copy de mon df pour divers test
df_knn = df.copy() # Test effectué pour la recherche de valeurs manquante
df_kmeans = df.copy() # Test effectué pour differéncier les vrai/faux billets

# Relations entre toutes de variables
sns.pairplot(df, hue = "is_genuine", palette="viridis")
plt.show()

##### margin_low et length permettent de bien distinguer les catégories de billets

df.is_genuine.value_counts().plot(kind = "pie")

## Algorithme des K-means

# Variables explicatives
X = df_kmeans[["margin_low", "length"]]

# Initialisation et ajustement du modèle K-Means avec 2 clusters
kmeans = KMeans(n_clusters=2, random_state=102023)
kmeans.fit(X)

# Labels de cluster pour chaque point de données
labels = kmeans.labels_

# Coordonnées des centroides
centroids = kmeans.cluster_centers_

# Labels de cluster au DataFrame original
df_kmeans["cluster"] = labels

# Résultats
cluster_counts = df_kmeans["cluster"].value_counts()
print(cluster_counts)

# Visualisation des clusters avec les centroides
plt.scatter(X["margin_low"], X["length"], c=labels, cmap="plasma")
plt.scatter(centroids[:, 0], centroids[:, 1], marker="x", s=200, linewidths=3, color="red", label="Centroides")
plt.xlabel("Margin Low")
plt.ylabel("Length")
plt.title("Clusters K-Means et visualisation des Centroides")
plt.legend()
plt.show()

kmeans0 = df_kmeans[df_kmeans["cluster"] == 0]
kmeans1 = df_kmeans[df_kmeans["cluster"] == 1]

for i in range(0, 2):
    print("*Cluster*", i)
    list_pays = df_kmeans[df_kmeans["cluster"] == i]
    for index in list_pays.index.tolist():
        print(index)
    print()

silhouette_avg = silhouette_score(X, labels)
print(f"Silhouette Score: {silhouette_avg}")

n_clusters = 2
sample_silhouette_values = silhouette_samples(X, labels)

fig, ax = plt.subplots(1, 1)
y_lower = 10

for i in range(n_clusters):
    ith_cluster_silhouette_values = sample_silhouette_values[labels == i]

    ith_cluster_silhouette_values.sort()
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = cm.nipy_spectral(float(i) / n_clusters)
    ax.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
    y_lower = y_upper + 10

ax.set_xlabel("Valeur du coefficient de silhouette")
ax.set_ylabel("Cluster label")
ax.axvline(x=silhouette_avg, color="red", linestyle="--")
ax.set_yticks([])  # Clear the yaxis labels / ticks
ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

plt.show()

##### Bien que le résultat soit plus proche de 1 que de 0, on peut le considérer comme bon. Cependant, je vais m'orienter vers la régression logistique.

X = df.drop("is_genuine", axis=1)
y = df["is_genuine"]

# Mise à l'échelle des données
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Création des données pour l'entrainement (80% pour l'entraînement, 20% pour les tests)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=102023)

# Régression logistique
logreg_model = LogisticRegression()
logreg_model.fit(X_train, y_train)

# Prédiction des étiquettes de test
y_pred = logreg_model.predict(X_test)

# Exactitude
accuracy = metrics.accuracy_score(y_test, y_pred)
print("Score d'exactitude : ", round(accuracy * 100, 1), "%")

# Matrice de confusion
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
print("Matrice de confusion :")
print(confusion_matrix)

# Courbe ROC
fpr, tpr, _ = metrics.roc_curve(y_test, logreg_model.predict_proba(X_test)[:, 1])
auc_score = metrics.roc_auc_score(y_test, logreg_model.predict_proba(X_test)[:, 1])

plt.plot(fpr, tpr, label="AUC = " + str(auc_score))
plt.legend()
plt.title("Courbe ROC")
plt.xlabel("Tx de faux positifs")
plt.ylabel("Tx de vrais positifs")
plt.show()

##### La régression logistique, avec son score d'exactitude de 99.7 %, surpasse nettement les résultats obtenus par l'algorithme K-Means en termes de précision de classification pour classer des billets.

##### Un AUC de 0.99 indique que le modèle a une excellente capacité à classer les billets vrais et faux.

counts = df["is_genuine"].value_counts()
print(counts)

df.info()

# Fonction d'importation (V1)

def scan_billets(nom_fichier_csv, modele):
    nvelle_echantillons = pd.read_csv(nom_fichier_csv)

    scaler = StandardScaler()
    nvo_echantillons_scaled = scaler.fit_transform(nvelle_echantillons.drop("is_genuine", axis=1))

    predictions = modele.predict(nvo_echantillons_scaled)

    return predictions

modele_logistique = logreg_model
nvelle_detection = scan_billets("test_fichier.csv", modele_logistique)
print(nvelle_detection)

# Fonction d'importation du modèle entrainé (v2)

def scan_billets(nom_fichier_csv, modele):
    # Charger le fichier CSV avec seulement les colonnes requises
    colonnes_requises = ["diagonal", "height_left", "height_right", "margin_low", "margin_up", "length"]
    nvelle_echantillons = pd.read_csv(nom_fichier_csv, usecols=colonnes_requises)

    scaler = StandardScaler()
    nvo_echantillons_scaled = scaler.fit_transform(nvelle_echantillons)

    predictions = modele.predict(nvo_echantillons_scaled)

    return predictions

modele_logistique = logreg_model
nvelle_detection = scan_billets("test_fichier.csv", modele_logistique)
print(nvelle_detection)

# Fonction d'importation d'échantillon pour tester le modèle entrainé (v2.14)

def scan_billets(nom_fichier_csv, modele):
    # Importation du fichier et séléction des colonnes nécessaire
    colonnes_requises = ["diagonal", "height_left", "height_right", "margin_low", "margin_up", "length"]
    nvelle_echantillons = pd.read_csv(nom_fichier_csv, usecols=colonnes_requises)

    scaler = StandardScaler()
    nvo_echantillons_scaled = scaler.fit_transform(nvelle_echantillons)

    predictions = modele.predict(nvo_echantillons_scaled)

    # Graphique en barplot pour les prédictions
    unique, counts = np.unique(predictions, return_counts=True)
    plt.bar(unique, counts, tick_label=["Faux", "Vrai"])
    plt.xlabel("Classé")
    plt.ylabel("Nombre de billets")
    plt.title("Scan des billets")

    # Nombre de billets catégorisé
    for i in range(len(unique)):
        plt.text(unique[i], counts[i], str(counts[i]), ha="center", va="bottom")

    plt.show()

    # Récupération des ID pour connaitre les vrai/faux billets
    df_id = pd.read_csv(nom_fichier_csv, usecols=["id"])
    df_id["is_genuine"] = predictions
    id_true = df_id[df_id["is_genuine"] == 1]['id']
    id_false = df_id[df_id["is_genuine"] == 0]['id']

    print("ID classés comme 'Vrai billets' :", id_true.tolist())
    print("ID classés comme 'Faux Billets' :", id_false.tolist())

modele_logistique = logreg_model
nvelle_detection = scan_billets("test_fichier.csv", modele_logistique)

# Chargement du modèle entrainé
modele_logistique = LogisticRegression()

# Définition de la fonction "scan_billets"
def scan_billets(nom_fichier_csv, modele):
    colonnes_requises = ["diagonal", "height_left", "height_right", "margin_low", "margin_up", "length"]
    nvelle_echantillons = pd.read_csv(nom_fichier_csv, usecols=colonnes_requises)

    scaler = StandardScaler()
    nvo_echantillons_scaled = scaler.fit_transform(nvelle_echantillons)

    predictions = modele.predict(nvo_echantillons_scaled)

    return predictions

# Application Streamlit
st.title("Détection des Faux Billets")

# Sidebar pour le téléchargement du fichier
st.sidebar.header("Télécharger le fichier CSV")
uploaded_file = st.sidebar.file_uploader("Téléchargez le fichier CSV contenant les échantillons à tester :")

if uploaded_file is not None:
    
    # Lancement et avancement du scan
    if st.sidebar.button("Lancer la Détection"):
        st.sidebar.text("Détection en cours...")
        
        # Appel de la fonction
        predictions = scan_billets(uploaded_file, modele_logistique)
        
        # Affichage des résultats
        st.subheader("Résultats de la Détection")
        unique, counts = np.unique(predictions, return_counts=True)
        st.bar_chart(pd.DataFrame({'Classé': ['Faux', 'Vrai'], 'Nombre de billets': counts}))
        
        # Récupération des ID pour connaitre les vrai/faux billets
        df_id = pd.read_csv(uploaded_file, usecols=["id"])
        df_id["is_genuine"] = predictions
        id_true = df_id[df_id["is_genuine"] == 1]['id']
        id_false = df_id[df_id["is_genuine"] == 0]['id']

        st.subheader("ID classés comme 'Vrai billets' :")
        st.write(id_true.tolist())

        st.subheader("ID classés comme 'Faux Billets' :")
        st.write(id_false.tolist())



